---
title: "CSP571 Project Data Prep and Transformations"
author: "Khushin Patel"
output:
  html_notebook:
    toc: yes
    toc_float: yes
  html_document:
    toc: yes
    df_print: paged
  pdf_document:
    toc: yes
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

### Value Checking -Khush
```{r}
# lets see the first 10 of the data we're working with
main_data = read.csv("./Data Files/data_aal.csv")
head(main_data, 10)
```

### Select the features and the predictor variable out of the data. We will also take the change column as net_change. Let's also add a categorical vaiable called cat_change that tells us if the change is positive(1) or negative(0)
```{r}
predictor_columns <- c("moon", "slash", "plus", "star", "two", "lunar", "solar", "flare")
processed_data = main_data[, predictor_columns]
processed_data$net_change = main_data$change
processed_data$cat_change = main_data$change_cat
head(processed_data, 10)
```

### Let's now see the distribution of the positive(1) class and the negative class in the predictors
```{r} 

frequency_tables <- lapply(processed_data[,predictor_columns], table)
frequency_tables
```


### All of the predictors are heavily imbalanced we first have to balance them
#### To address this imbalance I will take the number of rows where any of the predictor columns have a minority value of 1 and sample the same number of rows from the majority class(0)

```{r}
imbalanced_predictors = c("moon", "slash", "plus", "star", "two", "lunar", "solar", "flare")

# GPT generated code
majority_class <- subset(processed_data, rowSums(processed_data[imbalanced_predictors]) == 0)
minority_class <- subset(processed_data, rowSums(processed_data[imbalanced_predictors]) > 0)

undersampled_majority <- majority_class[sample(nrow(majority_class), nrow(minority_class), replace = FALSE), ]

# Combine undersampled majority class and minority class
balanced_data <- rbind(undersampled_majority, minority_class)
head(balanced_data, 10)
# lets see how the data is distributed now in our balanced dataset and shuffle it 
balanced_data <- balanced_data[sample(nrow(balanced_data)), ]
frequency_tables <- lapply(balanced_data[,imbalanced_predictors], table)
frequency_tables
```

#### It is still quite imbalanced but it is better than before but if we undersample the majority class even more, we might lose too much information. We've already lost a lot of it by undersampling the majority class.

#### Lets now see the correlations here between the undersampled predictors and the target

```{r}
library(psych)
columns_for_pairs <- c(imbalanced_predictors, "net_change")
# Create pairs plot
pairs.panels(balanced_data[columns_for_pairs], main = "Pairs Plot of Balanced Columns vs. net_change")
```
#### It seems like slash is the only one that has even a small correlation with net_change all the other ones are very faintly correlated. Maybe we will see a correlation if we combine them all into just one variable that signifies that there was some solar event occuring on that day. Lets first consider how it is distributed if we do all of the initial rows

```{r}
processed_data$has_event = ifelse(rowSums(processed_data[, imbalanced_predictors]) > 0, 1, 0)
head(processed_data, 10)

```
#### lets see if this shows any correlations
```{r}
columns_for_pairs <- c("has_event", "net_change")
# Create pairs plot
pairs.panels(processed_data[columns_for_pairs], main = "Has_event Columns vs. net_change")
```
#### still doesn't seem good lets try the same thing with the balanced dataset
```{r}
balanced_data$has_event = ifelse(rowSums(balanced_data[, imbalanced_predictors]) > 0, 1, 0)
head(balanced_data, 10)

pairs.panels(balanced_data[columns_for_pairs], main = "Has_event of Balanced Columns vs. net_change")
```
#### Similarly We don't see any significant correlation here
### Lets try to see if we can see a correlation we use categorical change as the target in this using the imbalanced dataset

```{r}
columns_for_pairs <- c(imbalanced_predictors, "has_event", "cat_change")
# Create pairs plot
pairs.panels(processed_data[columns_for_pairs], main = "Pairs Plot of Columns vs. Cat_change")
```

#### still seems like barely any correlation
#### now lets see if similary we can get something out of the balanced dataset

```{r}
predictor_columns <- c("moon", "slash",  "plus", "star", "two", "lunar", "solar", "flare")

columns_for_pairs <- c(imbalanced_predictors, "has_event", "cat_change")
# Create pairs plot
pairs.panels(balanced_data[columns_for_pairs], main = "Pairs Plot of Balanced Columns  vs. Cat_change")

```

### Models: Decision Tree
#### We will build a decision tree model that will use values of the predictor_columns to predict cat_change
##### Lets first split both the balanced and imbalanced datasets into their training(80%) and testing(20%) indices. We will also only use the predictor columns for predictors in the tree
```{r}
train_indices <- sample(1:nrow(balanced_data), size = round(0.8 * nrow(balanced_data)))
train_balanced_data <- balanced_data[train_indices, c(predictor_columns, "cat_change")]
test_balanced_data <- balanced_data[-train_indices, c(predictor_columns, "cat_change")]

train_indices <- sample(1:nrow(processed_data), size = round(.8 * nrow(processed_data)))
train_processed_data <- processed_data[train_indices, c(predictor_columns, "cat_change")]
test_processed_data <- processed_data[-train_indices, c(predictor_columns, "cat_change")]
```

##### Now lets build a decision tree on both datasets
```{r}
library(rpart)
library(rpart.plot)

tree_balanced <- rpart(cat_change ~ ., data = train_balanced_data, method = "class")
rpart.plot(tree_balanced, main = "Balanced Decision Tree for AAL")

tree_processed <- rpart(cat_change ~ ., data = train_processed_data, method = "class")
rpart.plot(tree_processed, main = "Regular Decision Tree for AAL")
```

```{r}
predictions <- predict(tree_balanced, test_balanced_data, type = "class")

# Calculate accuracy
accuracy <- mean(predictions == test_balanced_data$cat_change)
cat("Accuracy:", accuracy, "\n")

# Calculate confusion matrix
confusion_matrix <- table(Predicted = predictions, Actual = test_balanced_data$cat_change)

# Calculate precision, recall, and F1-score
precision <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
recall <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
f1_score <- 2 * precision * recall / (precision + recall)

cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-score:", f1_score, "\n")

confusion_matrix
```

### Models: Random Forest
#### We will build a random forest model that will use values of the predictor_columns to predict cat_change

```{r}
library(randomForest)
library(randomForestExplainer)

rf_model <- randomForest(cat_change ~ ., data = train_balanced_data)
importance <- importance(rf_model)
importance
# Plot feature importance
varImpPlot(rf_model, main = "Unbalanced Random Forest AAL")
```

```{r}
unique(predict(rf_model, processed_data))
```

```{r}
predictions <- round(predict(rf_model, test_balanced_data))

# Calculate accuracy
accuracy <- mean(predictions == test_balanced_data$cat_change)
cat("Accuracy:", accuracy, "\n")

# Calculate confusion matrix
confusion_matrix <- table(Predicted = predictions, Actual = test_balanced_data$cat_change)

# Calculate precision, recall, and F1-score
precision <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
recall <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
f1_score <- 2 * precision * recall / (precision + recall)

cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-score:", f1_score, "\n")

confusion_matrix
```
```{r}
table(balanced_data$cat_change)
```
Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file). 

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
