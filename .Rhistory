confusion_matrix <- table(Predicted = predictions, Actual = processed_data$cat_change)
# Calculate precision, recall, and F1-score
precision <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
recall <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
f1_score <- 2 * precision * recall / (precision + recall)
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-score:", f1_score, "\n")
confusion_matrix
# lets see the first 10 of the data we're working with
main_data = read.csv("./Data Files/data_sp.csv")
head(main_data, 10)
predictor_columns <- c("moon", "slash", "plus", "star", "two", "lunar", "solar", "flare")
processed_data = main_data[, predictor_columns]
processed_data$net_change = main_data$change
processed_data$cat_change = main_data$change_cat
head(processed_data, 10)
frequency_tables <- lapply(processed_data[,predictor_columns], table)
frequency_tables
imbalanced_predictors = c("moon", "slash", "plus", "star", "two", "lunar", "solar", "flare")
# GPT generated code
majority_class <- subset(processed_data, rowSums(processed_data[imbalanced_predictors]) == 0)
minority_class <- subset(processed_data, rowSums(processed_data[imbalanced_predictors]) > 0)
undersampled_majority <- majority_class[sample(nrow(majority_class), nrow(minority_class), replace = FALSE), ]
# Combine undersampled majority class and minority class
balanced_data <- rbind(undersampled_majority, minority_class)
head(balanced_data, 10)
# lets see how the data is distributed now in our balanced dataset and shuffle it
balanced_data <- balanced_data[sample(nrow(balanced_data)), ]
frequency_tables <- lapply(balanced_data[,imbalanced_predictors], table)
frequency_tables
library(psych)
columns_for_pairs <- c(imbalanced_predictors, "net_change")
# Create pairs plot
pairs.panels(balanced_data[columns_for_pairs], main = "Pairs Plot of Columns vs. net_change")
processed_data$has_event = ifelse(rowSums(processed_data[, imbalanced_predictors]) > 0, 1, 0)
head(processed_data, 10)
columns_for_pairs <- c("has_event", "net_change")
# Create pairs plot
pairs.panels(processed_data[columns_for_pairs], main = "Pairs Plot of Columns vs. net_change")
balanced_data$has_event = ifelse(rowSums(balanced_data[, imbalanced_predictors]) > 0, 1, 0)
head(balanced_data, 10)
pairs.panels(balanced_data[columns_for_pairs], main = "Pairs Plot of Columns vs. net_change")
columns_for_pairs <- c(imbalanced_predictors, "has_event", "cat_change")
# Create pairs plot
pairs.panels(processed_data[columns_for_pairs], main = "Pairs Plot of Columns vs. Cat_change")
predictor_columns <- c("moon", "slash",  "plus", "star", "two", "lunar", "solar", "flare")
columns_for_pairs <- c(imbalanced_predictors, "has_event", "cat_change")
# Create pairs plot
pairs.panels(balanced_data[columns_for_pairs], main = "Pairs Plot of Balanced Columns  vs. Cat_change")
train_indices <- sample(1:nrow(balanced_data), size = round(0.8 * nrow(balanced_data)))
train_balanced_data <- balanced_data[train_indices, c(predictor_columns, "cat_change")]
test_balanced_data <- balanced_data[-train_indices, c(predictor_columns, "cat_change")]
train_indices <- sample(1:nrow(processed_data), size = round(.8 * nrow(processed_data)))
train_processed_data <- processed_data[train_indices, c(predictor_columns, "cat_change")]
test_processed_data <- processed_data[-train_indices, c(predictor_columns, "cat_change")]
library(rpart)
library(rpart.plot)
tree_balanced <- rpart(cat_change ~ ., data = train_balanced_data, method = "class")
rpart.plot(tree_balanced)
tree_processed <- rpart(cat_change ~ ., data = train_processed_data, method = "class")
rpart.plot(tree_processed)
predictions <- predict(tree_balanced, test_balanced_data, type = "class")
# Calculate accuracy
accuracy <- mean(predictions == test_balanced_data$cat_change)
cat("Accuracy:", accuracy, "\n")
# Calculate confusion matrix
confusion_matrix <- table(Predicted = predictions, Actual = test_balanced_data$cat_change)
# Calculate precision, recall, and F1-score
precision <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
recall <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
f1_score <- 2 * precision * recall / (precision + recall)
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-score:", f1_score, "\n")
confusion_matrix
library(randomForest)
library(randomForestExplainer)
rf_model <- randomForest(cat_change ~ .-net_change, data = processed_data, method = "class")
importance <- importance(rf_model)
importance
# Plot feature importance
varImpPlot(rf_model)
predictions <- round(predict(rf_model, processed_data, type = "class"))
# Calculate accuracy
accuracy <- mean(predictions == processed_data$cat_change)
cat("Accuracy:", accuracy, "\n")
# Calculate confusion matrix
confusion_matrix <- table(Predicted = predictions, Actual = processed_data$cat_change)
# Calculate precision, recall, and F1-score
precision <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
recall <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
f1_score <- 2 * precision * recall / (precision + recall)
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-score:", f1_score, "\n")
confusion_matrix
library(randomForest)
library(randomForestExplainer)
rf_model <- randomForest(cat_change ~ .-net_change, data = processed_data, method = "class")
importance <- importance(rf_model)
importance
# Plot feature importance
varImpPlot(rf_model)
predictions <- round(predict(rf_model, processed_data, type = "class"))
# Calculate accuracy
accuracy <- mean(predictions == processed_data$cat_change)
cat("Accuracy:", accuracy, "\n")
# Calculate confusion matrix
confusion_matrix <- table(Predicted = predictions, Actual = processed_data$cat_change)
# Calculate precision, recall, and F1-score
precision <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
recall <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
f1_score <- 2 * precision * recall / (precision + recall)
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-score:", f1_score, "\n")
confusion_matrix
predictions <- round(predict(rf_model, processed_data, type = "class"))
# Calculate accuracy
accuracy <- mean(predictions == processed_data$cat_change)
cat("Accuracy:", accuracy, "\n")
# Calculate confusion matrix
confusion_matrix <- table(Predicted = predictions, Actual = processed_data$cat_change)
# Calculate precision, recall, and F1-score
precision <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
recall <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
f1_score <- 2 * precision * recall / (precision + recall)
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-score:", f1_score, "\n")
confusion_matrix
library(randomForest)
library(randomForestExplainer)
rf_model <- randomForest(cat_change ~ .-net_change, data = processed_data, method = "class")
importance <- importance(rf_model)
importance
# Plot feature importance
varImpPlot(rf_model)
predictions <- round(predict(rf_model, processed_data, type = "class"))
# Calculate accuracy
accuracy <- mean(predictions == processed_data$cat_change)
cat("Accuracy:", accuracy, "\n")
# Calculate confusion matrix
confusion_matrix <- table(Predicted = predictions, Actual = processed_data$cat_change)
# Calculate precision, recall, and F1-score
precision <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
recall <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
f1_score <- 2 * precision * recall / (precision + recall)
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-score:", f1_score, "\n")
confusion_matrix
library(randomForest)
library(randomForestExplainer)
rf_model <- randomForest(cat_change ~ .-net_change, data = processed_data, method = "class")
importance <- importance(rf_model)
importance
# Plot feature importance
varImpPlot(rf_model)
predictions <- round(predict(rf_model, processed_data, type = "class"))
# Calculate accuracy
accuracy <- mean(predictions == processed_data$cat_change)
cat("Accuracy:", accuracy, "\n")
# Calculate confusion matrix
confusion_matrix <- table(Predicted = predictions, Actual = processed_data$cat_change)
# Calculate precision, recall, and F1-score
precision <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
recall <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
f1_score <- 2 * precision * recall / (precision + recall)
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-score:", f1_score, "\n")
confusion_matrix
library(randomForest)
library(randomForestExplainer)
rf_model <- randomForest(cat_change ~ .-net_change, data = processed_data, method = "class")
importance <- importance(rf_model)
importance
# Plot feature importance
varImpPlot(rf_model)
predictions <- round(predict(rf_model, processed_data, type = "class"))
# Calculate accuracy
accuracy <- mean(predictions == processed_data$cat_change)
cat("Accuracy:", accuracy, "\n")
# Calculate confusion matrix
confusion_matrix <- table(Predicted = predictions, Actual = processed_data$cat_change)
# Calculate precision, recall, and F1-score
precision <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
recall <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
f1_score <- 2 * precision * recall / (precision + recall)
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-score:", f1_score, "\n")
confusion_matrix
library(randomForest)
library(randomForestExplainer)
rf_model <- randomForest(cat_change ~ ., data = balanced_data, method = "class")
importance <- importance(rf_model)
importance
# Plot feature importance
varImpPlot(rf_model)
library(randomForest)
library(randomForestExplainer)
rf_model <- randomForest(cat_change ~ .-net_change, data = balanced_data, method = "class")
importance <- importance(rf_model)
importance
# Plot feature importance
varImpPlot(rf_model)
library(randomForest)
library(randomForestExplainer)
rf_model <- randomForest(cat_change ~ ., data = train_balanced_data, method = "class")
importance <- importance(rf_model)
importance
# Plot feature importance
varImpPlot(rf_model)
predictions <- round(predict(rf_model, train_test_data, type = "class"))
predictions <- round(predict(rf_model, test_balanced_data, type = "class"))
# Calculate accuracy
accuracy <- mean(predictions == test_balanced_data$cat_change)
cat("Accuracy:", accuracy, "\n")
# Calculate confusion matrix
confusion_matrix <- table(Predicted = predictions, Actual = test_balanced_data$cat_change)
# Calculate precision, recall, and F1-score
precision <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
recall <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
f1_score <- 2 * precision * recall / (precision + recall)
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-score:", f1_score, "\n")
confusion_matrix
predictions <- round(predict(rf_model, processed_data, type = "class"))
# Calculate accuracy
accuracy <- mean(predictions == processed_data$cat_change)
cat("Accuracy:", accuracy, "\n")
# Calculate confusion matrix
confusion_matrix <- table(Predicted = predictions, Actual = processed_data$cat_change)
# Calculate precision, recall, and F1-score
precision <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
recall <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
f1_score <- 2 * precision * recall / (precision + recall)
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-score:", f1_score, "\n")
confusion_matrix
library(randomForest)
library(randomForestExplainer)
rf_model <- randomForest(cat_change ~ .-net_change, data = processed_data, method = "class")
importance <- importance(rf_model)
importance
# Plot feature importance
varImpPlot(rf_model)
predictions <- round(predict(rf_model, processed_data, type = "class"))
# Calculate accuracy
accuracy <- mean(predictions == processed_data$cat_change)
cat("Accuracy:", accuracy, "\n")
# Calculate confusion matrix
confusion_matrix <- table(Predicted = predictions, Actual = processed_data$cat_change)
# Calculate precision, recall, and F1-score
precision <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
recall <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
f1_score <- 2 * precision * recall / (precision + recall)
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-score:", f1_score, "\n")
confusion_matrix
predictions <- round(predict(rf_model, processed_data, type = "class"))
# Calculate accuracy
accuracy <- mean(predictions == processed_data$cat_change)
cat("Accuracy:", accuracy, "\n")
# Calculate confusion matrix
confusion_matrix <- table(Predicted = predictions, Actual = processed_data$cat_change)
# Calculate precision, recall, and F1-score
precision <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
recall <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
f1_score <- 2 * precision * recall / (precision + recall)
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-score:", f1_score, "\n")
confusion_matrix
predictions
predict(rf_model, processed_data, type = "class")
predictions <- predict(tree_balanced, test_balanced_data, type = "class")
# Calculate accuracy
accuracy <- mean(predictions == test_balanced_data$cat_change)
cat("Accuracy:", accuracy, "\n")
# Calculate confusion matrix
confusion_matrix <- table(Predicted = predictions, Actual = test_balanced_data$cat_change)
# Calculate precision, recall, and F1-score
precision <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
recall <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
f1_score <- 2 * precision * recall / (precision + recall)
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-score:", f1_score, "\n")
confusion_matrix
predictions
predictions <- predict(tree_balanced, test_balanced_data)
# Calculate accuracy
accuracy <- mean(predictions == test_balanced_data$cat_change)
cat("Accuracy:", accuracy, "\n")
# Calculate confusion matrix
confusion_matrix <- table(Predicted = predictions, Actual = test_balanced_data$cat_change)
predictions
predictions <- predict(tree_balanced, test_balanced_data, type = "class")
# Calculate accuracy
accuracy <- mean(predictions == test_balanced_data$cat_change)
cat("Accuracy:", accuracy, "\n")
# Calculate confusion matrix
confusion_matrix <- table(Predicted = predictions, Actual = test_balanced_data$cat_change)
# Calculate precision, recall, and F1-score
precision <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
recall <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
f1_score <- 2 * precision * recall / (precision + recall)
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-score:", f1_score, "\n")
confusion_matrix
library(randomForest)
library(randomForestExplainer)
rf_model <- randomForest(cat_change ~ .-net_change, data = processed_data)
importance <- importance(rf_model)
importance
# Plot feature importance
varImpPlot(rf_model)
predict(rf_model, processed_data)
unique(predict(rf_model, processed_data))
predictions <- round(predict(rf_model, processed_data))
# Calculate accuracy
accuracy <- mean(predictions == processed_data$cat_change)
cat("Accuracy:", accuracy, "\n")
# Calculate confusion matrix
confusion_matrix <- table(Predicted = predictions, Actual = processed_data$cat_change)
# Calculate precision, recall, and F1-score
precision <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
recall <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
f1_score <- 2 * precision * recall / (precision + recall)
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-score:", f1_score, "\n")
confusion_matrix
predictions
predictions <- round(predict(rf_model, processed_data))
# Calculate accuracy
accuracy <- mean(predictions == processed_data$cat_change)
cat("Accuracy:", accuracy, "\n")
# Calculate confusion matrix
confusion_matrix <- table(Predicted = predictions, Actual = processed_data$cat_change)
# Calculate precision, recall, and F1-score
precision <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
recall <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
f1_score <- 2 * precision * recall / (precision + recall)
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-score:", f1_score, "\n")
confusion_matrix
table(processed_data$cat_change)
library(rpart)
library(rpart.plot)
tree_balanced <- rpart(cat_change ~ ., data = train_balanced_data, method = "class")
rpart.plot(tree_balanced)
tree_processed <- rpart(cat_change ~ ., data = train_processed_data, method = "class")
rpart.plot(tree_processed)
predictions <- predict(tree_balanced, test_balanced_data, type = "class")
# Calculate accuracy
accuracy <- mean(predictions == test_balanced_data$cat_change)
cat("Accuracy:", accuracy, "\n")
# Calculate confusion matrix
confusion_matrix <- table(Predicted = predictions, Actual = test_balanced_data$cat_change)
# Calculate precision, recall, and F1-score
precision <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
recall <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
f1_score <- 2 * precision * recall / (precision + recall)
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-score:", f1_score, "\n")
confusion_matrix
# lets see the first 10 of the data we're working with
main_data = read.csv("./Data Files/data_aal.csv")
head(main_data, 10)
predictor_columns <- c("moon", "slash", "plus", "star", "two", "lunar", "solar", "flare")
processed_data = main_data[, predictor_columns]
processed_data$net_change = main_data$change
processed_data$cat_change = main_data$change_cat
head(processed_data, 10)
frequency_tables <- lapply(processed_data[,predictor_columns], table)
frequency_tables
imbalanced_predictors = c("moon", "slash", "plus", "star", "two", "lunar", "solar", "flare")
# GPT generated code
majority_class <- subset(processed_data, rowSums(processed_data[imbalanced_predictors]) == 0)
minority_class <- subset(processed_data, rowSums(processed_data[imbalanced_predictors]) > 0)
undersampled_majority <- majority_class[sample(nrow(majority_class), nrow(minority_class), replace = FALSE), ]
# Combine undersampled majority class and minority class
balanced_data <- rbind(undersampled_majority, minority_class)
head(balanced_data, 10)
# lets see how the data is distributed now in our balanced dataset and shuffle it
balanced_data <- balanced_data[sample(nrow(balanced_data)), ]
frequency_tables <- lapply(balanced_data[,imbalanced_predictors], table)
frequency_tables
library(psych)
columns_for_pairs <- c(imbalanced_predictors, "net_change")
# Create pairs plot
pairs.panels(balanced_data[columns_for_pairs], main = "Pairs Plot of Columns vs. net_change")
processed_data$has_event = ifelse(rowSums(processed_data[, imbalanced_predictors]) > 0, 1, 0)
head(processed_data, 10)
columns_for_pairs <- c("has_event", "net_change")
# Create pairs plot
pairs.panels(processed_data[columns_for_pairs], main = "Pairs Plot of Columns vs. net_change")
balanced_data$has_event = ifelse(rowSums(balanced_data[, imbalanced_predictors]) > 0, 1, 0)
head(balanced_data, 10)
pairs.panels(balanced_data[columns_for_pairs], main = "Pairs Plot of Columns vs. net_change")
columns_for_pairs <- c(imbalanced_predictors, "has_event", "cat_change")
# Create pairs plot
pairs.panels(processed_data[columns_for_pairs], main = "Pairs Plot of Columns vs. Cat_change")
predictor_columns <- c("moon", "slash",  "plus", "star", "two", "lunar", "solar", "flare")
columns_for_pairs <- c(imbalanced_predictors, "has_event", "cat_change")
# Create pairs plot
pairs.panels(balanced_data[columns_for_pairs], main = "Pairs Plot of Balanced Columns  vs. Cat_change")
train_indices <- sample(1:nrow(balanced_data), size = round(0.8 * nrow(balanced_data)))
train_balanced_data <- balanced_data[train_indices, c(predictor_columns, "cat_change")]
test_balanced_data <- balanced_data[-train_indices, c(predictor_columns, "cat_change")]
train_indices <- sample(1:nrow(processed_data), size = round(.8 * nrow(processed_data)))
train_processed_data <- processed_data[train_indices, c(predictor_columns, "cat_change")]
test_processed_data <- processed_data[-train_indices, c(predictor_columns, "cat_change")]
library(rpart)
library(rpart.plot)
tree_balanced <- rpart(cat_change ~ ., data = train_balanced_data, method = "class")
rpart.plot(tree_balanced)
tree_processed <- rpart(cat_change ~ ., data = train_processed_data, method = "class")
rpart.plot(tree_processed)
predictions <- predict(tree_balanced, test_balanced_data, type = "class")
# Calculate accuracy
accuracy <- mean(predictions == test_balanced_data$cat_change)
cat("Accuracy:", accuracy, "\n")
# Calculate confusion matrix
confusion_matrix <- table(Predicted = predictions, Actual = test_balanced_data$cat_change)
# Calculate precision, recall, and F1-score
precision <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
recall <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
f1_score <- 2 * precision * recall / (precision + recall)
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-score:", f1_score, "\n")
confusion_matrix
library(randomForest)
library(randomForestExplainer)
rf_model <- randomForest(cat_change ~ .-net_change, data = processed_data)
importance <- importance(rf_model)
importance
# Plot feature importance
varImpPlot(rf_model)
unique(predict(rf_model, processed_data))
predictions <- round(predict(rf_model, processed_data))
# Calculate accuracy
accuracy <- mean(predictions == processed_data$cat_change)
cat("Accuracy:", accuracy, "\n")
# Calculate confusion matrix
confusion_matrix <- table(Predicted = predictions, Actual = processed_data$cat_change)
# Calculate precision, recall, and F1-score
precision <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
recall <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
f1_score <- 2 * precision * recall / (precision + recall)
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-score:", f1_score, "\n")
confusion_matrix
table(processed_data$cat_change)
library(randomForest)
library(randomForestExplainer)
rf_model <- randomForest(cat_change ~ , data = train_balanced_data)
library(randomForest)
library(randomForestExplainer)
rf_model <- randomForest(cat_change ~ ., data = train_balanced_data)
importance <- importance(rf_model)
importance
# Plot feature importance
varImpPlot(rf_model)
predictions <- round(predict(rf_model, test_balanced_data))
# Calculate accuracy
accuracy <- mean(predictions == test_balanced_data$cat_change)
cat("Accuracy:", accuracy, "\n")
# Calculate confusion matrix
confusion_matrix <- table(Predicted = predictions, Actual = test_balanced_data$cat_change)
# Calculate precision, recall, and F1-score
precision <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
recall <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
f1_score <- 2 * precision * recall / (precision + recall)
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-score:", f1_score, "\n")
confusion_matrix
balanced_data$has_event = ifelse(rowSums(balanced_data[, imbalanced_predictors]) > 0, 1, 0)
head(balanced_data, 10)
pairs.panels(balanced_data[columns_for_pairs], main = "Has_event of Balanced Columns vs. net_change")
columns_for_pairs <- c("has_event", "net_change")
# Create pairs plot
pairs.panels(processed_data[columns_for_pairs], main = "Has_event Column vs. net_change")
balanced_data$has_event = ifelse(rowSums(balanced_data[, imbalanced_predictors]) > 0, 1, 0)
head(balanced_data, 10)
pairs.panels(balanced_data[columns_for_pairs], main = "Pairs Plot of Balanced Columns vs. net_change")
balanced_data$has_event = ifelse(rowSums(balanced_data[, imbalanced_predictors]) > 0, 1, 0)
head(balanced_data, 10)
pairs.panels(balanced_data[columns_for_pairs], main = "Has_event of Balanced Columns vs. net_change")
columns_for_pairs <- c("has_event", "net_change")
# Create pairs plot
pairs.panels(processed_data[columns_for_pairs], main = "Has_event Columns vs. net_change")
balanced_data$has_event = ifelse(rowSums(balanced_data[, imbalanced_predictors]) > 0, 1, 0)
head(balanced_data, 10)
pairs.panels(balanced_data[columns_for_pairs], main = "Has_event of Balanced has_event vs. net_change")
balanced_data$has_event = ifelse(rowSums(balanced_data[, imbalanced_predictors]) > 0, 1, 0)
head(balanced_data, 10)
pairs.panels(balanced_data[columns_for_pairs], main = "Has_event of Columns vs. net_change")
